{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: Yuvaraj Prem Kumar <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "References:\n",
    "-----------\n",
    "\n",
    "Ben Trevett (2020). PyTorch Seq2Seq. URL: https://github.com/bentrevett/pytorch-seq2seq\n",
    "\n",
    "Alexander Rush (2018). \\The Annotated Transformer\". In: Melbourne, Australia: Association for Computational Linguistics, pp. 52{60. doi: 10.18653/v1/W18-2509. URL: https://aclanthology.org/W18-2509\n",
    "\n",
    "Yu-Hsiang Huang (2019) Attention is all you need: A Pytorch Implementation URL: https://github.com/jadore801120/attention-is-all-you-need-pytorch\n",
    "\n",
    "TensorFlow (2021). Transformer model for language understanding. URL: https://www.tensorflow.org/text/tutorials/transformer.#\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset exploration for the Multi30K and IWSLT2016 dataset:\n",
    "\n",
    "* **Loading the dataset via TorchText in-built datasets.**\n",
    "* **Showing sample sentence pairs for the translation task.**\n",
    "* **Using the Spacy EN and DE language model for data preprocessing (tokenization).**\n",
    "* **Implementation for the PyTorch dataloader iterator.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from itertools import islice\n",
    "#plt.style.use('ggplot')\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchtext.datasets import Multi30k,IWSLT2016\n",
    "from torchtext.legacy.datasets import Multi30k, IWSLT\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#from torchtext.data import Field, BucketIterator\n",
    "#from torchtext.data.utils import get_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spacy's English and German language model, used for tokenization. More info here: https://spacy.io/usage/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and import the languages models. You may have to restart the runtime to refresh the language model\n",
    "#!python -m spacy download en_core_web_sm\n",
    "#!python -m spacy download de_core_news_sm\n",
    "\n",
    "import en_core_web_sm\n",
    "import de_core_news_sm\n",
    "nlp_en = en_core_web_sm.load()\n",
    "nlp_de = de_core_news_sm.load()\n",
    "\n",
    "# or:\n",
    "#nlp_en = spacy.load('en_core_web_sm')\n",
    "#nlp_de = spacy.load('de_core_news_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To left-align the output DataFrames, easier readability for the text data\n",
    "\n",
    "def left_align(df: pd.DataFrame):\n",
    "    left_aligned_df = df.style.set_properties(**{'text-align': 'left'})\n",
    "    left_aligned_df = left_aligned_df.set_table_styles(\n",
    "        [dict(selector='th', props=[('text-align', 'left')])]\n",
    "    )\n",
    "    return left_aligned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in nlp_de.tokenizer(text)]\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in nlp_en.tokenizer(text)]\n",
    "\n",
    "# German lang. is the source\n",
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "# English lang. is the target\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi30K dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = Multi30k(root='.data', \n",
    "                                             split=('train', 'valid', 'test'),\n",
    "                                             language_pair=('de', 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sentence pairs: 29000\n",
      "Number of validation sentence pairs: 1014\n",
      "Number of test sentence pairs: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of train sentence pairs: {len(train_iter)}\")\n",
    "print(f\"Number of validation sentence pairs: {len(valid_iter)}\") \n",
    "print(f\"Number of test sentence pairs: {len(test_iter)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_336a6084_292f_11ec_9f05_acde48001122 th {\n",
       "          text-align: left;\n",
       "    }#T_336a6084_292f_11ec_9f05_acde48001122row0_col0,#T_336a6084_292f_11ec_9f05_acde48001122row0_col1,#T_336a6084_292f_11ec_9f05_acde48001122row1_col0,#T_336a6084_292f_11ec_9f05_acde48001122row1_col1,#T_336a6084_292f_11ec_9f05_acde48001122row2_col0,#T_336a6084_292f_11ec_9f05_acde48001122row2_col1,#T_336a6084_292f_11ec_9f05_acde48001122row3_col0,#T_336a6084_292f_11ec_9f05_acde48001122row3_col1,#T_336a6084_292f_11ec_9f05_acde48001122row4_col0,#T_336a6084_292f_11ec_9f05_acde48001122row4_col1{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_336a6084_292f_11ec_9f05_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >DE</th>        <th class=\"col_heading level0 col1\" >EN</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_336a6084_292f_11ec_9f05_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row0_col0\" class=\"data row0 col0\" >Zwei junge weiße Männer sind im Freien in der Nähe vieler Büsche.\n",
       "</td>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row0_col1\" class=\"data row0 col1\" >Two young, White males are outside near many bushes.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_336a6084_292f_11ec_9f05_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row1_col0\" class=\"data row1 col0\" >Mehrere Männer mit Schutzhelmen bedienen ein Antriebsradsystem.\n",
       "</td>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row1_col1\" class=\"data row1 col1\" >Several men in hard hats are operating a giant pulley system.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_336a6084_292f_11ec_9f05_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row2_col0\" class=\"data row2 col0\" >Ein kleines Mädchen klettert in ein Spielhaus aus Holz.\n",
       "</td>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row2_col1\" class=\"data row2 col1\" >A little girl climbing into a wooden playhouse.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_336a6084_292f_11ec_9f05_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row3_col0\" class=\"data row3 col0\" >Ein Mann in einem blauen Hemd steht auf einer Leiter und putzt ein Fenster.\n",
       "</td>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row3_col1\" class=\"data row3 col1\" >A man in a blue shirt is standing on a ladder cleaning a window.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_336a6084_292f_11ec_9f05_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row4_col0\" class=\"data row4 col0\" >Zwei Männer stehen am Herd und bereiten Essen zu.\n",
       "</td>\n",
       "                        <td id=\"T_336a6084_292f_11ec_9f05_acde48001122row4_col1\" class=\"data row4 col1\" >Two men are at the stove preparing food.\n",
       "</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fafb9bd7910>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi30k_df = pd.DataFrame(list(islice(train_iter,5)), columns=['DE','EN'])\n",
    "left_align(multi30k_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE: Ein Mann in grün hält eine Gitarre, während der andere Mann sein Hemd ansieht.\n",
      " EN: A man in green holds a guitar while the other man observes his shirt.\n",
      "\n",
      "['Ein', 'Mann', 'in', 'grün', 'hält', 'eine', 'Gitarre', ',', 'während', 'der', 'andere', 'Mann', 'sein', 'Hemd', 'ansieht', '.', '\\n']\n",
      "['A', 'man', 'in', 'green', 'holds', 'a', 'guitar', 'while', 'the', 'other', 'man', 'observes', 'his', 'shirt', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "src_sentence, tgt_sentence = next(train_iter)\n",
    "print(f\"DE: {src_sentence} EN: {tgt_sentence}\")\n",
    "\n",
    "for doc in nlp_en.pipe([src_sentence]):\n",
    "    print([(ent.text) for ent in doc])\n",
    "\n",
    "for doc in nlp_de.pipe([tgt_sentence]):\n",
    "    print([(ent.text) for ent in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG), root='data')\n",
    "\n",
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_data_info(train_data, valid_data, test_data, src_field, trg_field):\n",
    "    \"\"\" This prints some useful stuff about our data sets. \"\"\"\n",
    "\n",
    "    print(\"Data set sizes (number of sentence pairs):\")\n",
    "    print('train', len(train_data))\n",
    "    print('valid', len(valid_data))\n",
    "    print('test', len(test_data), \"\\n\")\n",
    "\n",
    "    print(\"First training example:\")\n",
    "    print(\"src:\", \" \".join(vars(train_data[0])['src']))\n",
    "    print(\"trg:\", \" \".join(vars(train_data[0])['trg']), \"\\n\")\n",
    "\n",
    "    print(\"Most common words (src):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in src_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "    print(\"Most common words (trg):\")\n",
    "    print(\"\\n\".join([\"%10s %10d\" % x for x in trg_field.vocab.freqs.most_common(10)]), \"\\n\")\n",
    "\n",
    "    print(\"First 10 words (src):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(src_field.vocab.itos[:10])), \"\\n\")\n",
    "    print(\"First 10 words (trg):\")\n",
    "    print(\"\\n\".join(\n",
    "        '%02d %s' % (i, t) for i, t in enumerate(trg_field.vocab.itos[:10])), \"\\n\")\n",
    "\n",
    "    print(\"Number of German words (types):\", len(src_field.vocab))\n",
    "    print(\"Number of English words (types):\", len(trg_field.vocab), \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 29000\n",
      "valid 1014\n",
      "test 1000 \n",
      "\n",
      "First training example:\n",
      "src: zwei junge weiße männer sind im freien in der nähe vieler büsche .\n",
      "trg: two young , white males are outside near many bushes . \n",
      "\n",
      "Most common words (src):\n",
      "         .      28809\n",
      "       ein      18851\n",
      "     einem      13711\n",
      "        in      11895\n",
      "      eine       9909\n",
      "         ,       8938\n",
      "       und       8925\n",
      "       mit       8843\n",
      "       auf       8745\n",
      "      mann       7805 \n",
      "\n",
      "Most common words (trg):\n",
      "         a      49165\n",
      "         .      27623\n",
      "        in      14886\n",
      "       the      10955\n",
      "        on       8035\n",
      "       man       7781\n",
      "        is       7525\n",
      "       and       7379\n",
      "        of       6871\n",
      "      with       6179 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <sos>\n",
      "03 <eos>\n",
      "04 .\n",
      "05 ein\n",
      "06 einem\n",
      "07 in\n",
      "08 eine\n",
      "09 , \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <sos>\n",
      "03 <eos>\n",
      "04 a\n",
      "05 .\n",
      "06 in\n",
      "07 the\n",
      "08 on\n",
      "09 man \n",
      "\n",
      "Number of German words (types): 7853\n",
      "Number of English words (types): 5893 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IWSLT2016 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter, test_iter = IWSLT2016(root='.data',\n",
    "                                              split=('train', 'valid', 'test'),\n",
    "                                              language_pair=('de', 'en'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train sentence pairs: 196884\n",
      "Number of validation sentence pairs: 993\n",
      "Number of test sentence pairs: 1305\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of train sentence pairs: {len(train_iter)}\")\n",
    "print(f\"Number of validation sentence pairs: {len(valid_iter)}\") \n",
    "print(f\"Number of test sentence pairs: {len(test_iter)}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_433de890_2930_11ec_9f05_acde48001122 th {\n",
       "          text-align: left;\n",
       "    }#T_433de890_2930_11ec_9f05_acde48001122row0_col0,#T_433de890_2930_11ec_9f05_acde48001122row0_col1,#T_433de890_2930_11ec_9f05_acde48001122row1_col0,#T_433de890_2930_11ec_9f05_acde48001122row1_col1,#T_433de890_2930_11ec_9f05_acde48001122row2_col0,#T_433de890_2930_11ec_9f05_acde48001122row2_col1,#T_433de890_2930_11ec_9f05_acde48001122row3_col0,#T_433de890_2930_11ec_9f05_acde48001122row3_col1,#T_433de890_2930_11ec_9f05_acde48001122row4_col0,#T_433de890_2930_11ec_9f05_acde48001122row4_col1{\n",
       "            text-align:  left;\n",
       "        }</style><table id=\"T_433de890_2930_11ec_9f05_acde48001122\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >DE</th>        <th class=\"col_heading level0 col1\" >EN</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_433de890_2930_11ec_9f05_acde48001122level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row0_col0\" class=\"data row0 col0\" >David Gallo: Das ist Bill Lange. Ich bin Dave Gallo.\n",
       "</td>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row0_col1\" class=\"data row0 col1\" >David Gallo: This is Bill Lange. I'm Dave Gallo.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_433de890_2930_11ec_9f05_acde48001122level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row1_col0\" class=\"data row1 col0\" >Wir werden Ihnen einige Geschichten über das Meer in Videoform erzählen.\n",
       "</td>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row1_col1\" class=\"data row1 col1\" >And we're going to tell you some stories from the sea here in video.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_433de890_2930_11ec_9f05_acde48001122level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row2_col0\" class=\"data row2 col0\" >Wir haben ein paar der unglaublichsten Aufnahmen der Titanic, die man je gesehen hat,, und wir werden Ihnen nichts davon zeigen.\n",
       "</td>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row2_col1\" class=\"data row2 col1\" >We've got some of the most incredible video of Titanic that's ever been seen, and we're not going to show you any of it.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_433de890_2930_11ec_9f05_acde48001122level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row3_col0\" class=\"data row3 col0\" >Die Wahrheit ist, dass die Titanic – obwohl sie alle Kinokassenrekorde bricht – nicht gerade die aufregendste Geschichte vom Meer ist.\n",
       "</td>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row3_col1\" class=\"data row3 col1\" >The truth of the matter is that the Titanic -- even though it's breaking all sorts of box office records -- it's not the most exciting story from the sea.\n",
       "</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                        <th id=\"T_433de890_2930_11ec_9f05_acde48001122level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row4_col0\" class=\"data row4 col0\" >Ich denke, das Problem ist, dass wir das Meer für zu selbstverständlich halten.\n",
       "</td>\n",
       "                        <td id=\"T_433de890_2930_11ec_9f05_acde48001122row4_col1\" class=\"data row4 col1\" >And the problem, I think, is that we take the ocean for granted.\n",
       "</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7faf8b62b950>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iwslt_df = pd.DataFrame(list(islice(train_iter,5)), columns=['DE','EN'])\n",
    "left_align(iwslt_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DE: Wenn man darüber nachdenkt, machen die Ozeane 75 % des Planeten aus.\n",
      " EN: When you think about it, the oceans are 75 percent of the planet.\n",
      "\n",
      "['Wenn', 'man', 'darüber', 'nachdenkt', ',', 'machen', 'die', 'Ozeane', '75', '%', 'des', 'Planeten', 'aus', '.', '\\n']\n",
      "['When', 'you', 'think', 'about', 'it', ',', 'the', 'oceans', 'are', '75', 'percent', 'of', 'the', 'planet', '.', '\\n']\n"
     ]
    }
   ],
   "source": [
    "src_sentence, tgt_sentence = next(train_iter)\n",
    "print(f\"DE: {src_sentence} EN: {tgt_sentence}\")\n",
    "\n",
    "for doc in nlp_en.pipe([src_sentence]):\n",
    "    print([(ent.text) for ent in doc])\n",
    "\n",
    "for doc in nlp_de.pipe([tgt_sentence]):\n",
    "    print([(ent.text) for ent in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    \"\"\"\n",
    "    Tokenizes German text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in nlp_de.tokenizer(text)]\n",
    "\n",
    "def tokenize_en(text):\n",
    "    \"\"\"\n",
    "    Tokenizes English text from a string into a list of strings\n",
    "    \"\"\"\n",
    "    return [tok.text for tok in nlp_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize = tokenize_de, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)\n",
    "\n",
    "TRG = Field(tokenize = tokenize_en, \n",
    "            init_token = '<sos>', \n",
    "            eos_token = '<eos>', \n",
    "            lower = True, \n",
    "            batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data, test_data = IWSLT.splits(exts = ('.de', '.en'), \n",
    "                                                    fields = (SRC, TRG), root='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC.build_vocab(train_data, min_freq = 2)\n",
    "TRG.build_vocab(train_data, min_freq = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set sizes (number of sentence pairs):\n",
      "train 196884\n",
      "valid 993\n",
      "test 1305 \n",
      "\n",
      "First training example:\n",
      "src: david gallo : das ist bill lange . ich bin dave gallo .\n",
      "trg: david gallo : this is bill lange . i 'm dave gallo . \n",
      "\n",
      "Most common words (src):\n",
      "         ,     277689\n",
      "         .     201700\n",
      "       und      99754\n",
      "       die      91280\n",
      "       sie      61033\n",
      "       das      58977\n",
      "       ich      58472\n",
      "       der      57421\n",
      "       ist      51463\n",
      "       wir      49321 \n",
      "\n",
      "Most common words (trg):\n",
      "         ,     234067\n",
      "         .     194932\n",
      "       the     162091\n",
      "       and     115840\n",
      "        to      95881\n",
      "        of      89481\n",
      "         a      81801\n",
      "      that      73082\n",
      "         i      63755\n",
      "        in      60638 \n",
      "\n",
      "First 10 words (src):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <sos>\n",
      "03 <eos>\n",
      "04 ,\n",
      "05 .\n",
      "06 und\n",
      "07 die\n",
      "08 sie\n",
      "09 das \n",
      "\n",
      "First 10 words (trg):\n",
      "00 <unk>\n",
      "01 <pad>\n",
      "02 <sos>\n",
      "03 <eos>\n",
      "04 ,\n",
      "05 .\n",
      "06 the\n",
      "07 and\n",
      "08 to\n",
      "09 of \n",
      "\n",
      "Number of German words (types): 56378\n",
      "Number of English words (types): 32772 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_data_info(train_data, valid_data, test_data, SRC, TRG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "\n",
    "https://bastings.github.io/annotated_encoder_decoder/ <br>\n",
    "https://github.com/bentrevett/pytorch-seq2seq/blob/master/1%20-%20Sequence%20to%20Sequence%20Learning%20with%20Neural%20Networks.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
